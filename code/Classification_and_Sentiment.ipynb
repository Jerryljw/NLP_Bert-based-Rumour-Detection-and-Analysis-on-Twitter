{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Classification_and_Sentiment.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Colab Preparation, comment the following if you are running on local."],"metadata":{"id":"DRnsYZsJLHZh"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jUMVRAHbK8t9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654689241826,"user_tz":-480,"elapsed":27742,"user":{"displayName":"Yifan Deng","userId":"12140207404720131325"}},"outputId":"910ccdbd-645c-4ad4-9303-b2aab761390c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","drive  sample_data\n"," bactch8epoch10RoGrucell.dat   GPU_TokenSeq_Roberta_Gru.ipynb\n"," batch16epoch10grucell.dat     grucell_32.dat\n"," batch8epoch10ROGRU.dat        gru.ipynb\n"," batch8epoch4BERT.dat\t       logs\n"," batch8epoch4ROGRU.dat\t       maxpool_gru.dat\n"," batch8epoch4RoGruSeq.dat      param\n"," BERT_FC.ipynb\t\t       pickleData.ipynb\n"," covid_result.csv\t       project-data\n"," evidence512.pt\t\t       project.pdf\n"," evidence_highdrop_12.pt       proj.ipynb\n"," evidence_highL2_15.pt\t       report.gdoc\n"," evidence.ipynb\t\t       roberta_grucell.ipynb\n"," evidence+lastfour.ipynb       roberta_pretrained\n"," evidence+lastfour.pt\t       ro_mean_four_gru.dat\n"," glove\t\t\t      'RumourCLS and Sentiment.ipynb'\n"," GPU_BERT.ipynb\t\t       sentenceTransformer.ipynb\n"," GPU_Roberta_cnn.ipynb\t       seqgru.pt\n"," GPU_Roberta_gru.ipynb\t       simplegru+seqgru+bi.ipynb\n"," GPU_Roberta_gru_moxue.ipynb   submissioncsv\n"," GPU_SBert_Roberta_Gru.ipynb   test_text.csv\n"]}],"source":["import os\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","!ls\n","os.chdir('/content/drive/MyDrive/2022S1/90042nlp/proj')\n","!ls"]},{"cell_type":"code","source":["!pip install torchvision transformers\n","!pip install torch==1.11.0"],"metadata":{"id":"h-uCfiT0Lknh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data Preprocessing"],"metadata":{"id":"X3iKstwZLpS4"}},{"cell_type":"markdown","source":["Import and download related lib for text preprocessing."],"metadata":{"id":"9wWFUsCkLu1h"}},{"cell_type":"code","source":["import nltk\n","nltk.download(\"words\") \n","nltk.download(\"punkt\")\n","nltk.download(\"wordnet\")\n","nltk.download(\"stopwords\")\n","\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","from nltk.stem.wordnet import WordNetLemmatizer\n","from nltk.tokenize import TweetTokenizer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eXrpHnzSLuFR","outputId":"3d0b5194-92f6-4c2b-dc2e-0e0eaac3e5a7","executionInfo":{"status":"ok","timestamp":1654689247506,"user_tz":-480,"elapsed":2851,"user":{"displayName":"Yifan Deng","userId":"12140207404720131325"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/words.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]},{"cell_type":"markdown","source":["Preprocessing contains:\n","1. lower case\n","2. remove retweet format, hashtag, emoji\n","3. substitute @ and url to @user and http (especially for roberta)\n","4. tokenize and remove stopwords\n","5. lemmatize tokens\\\n","But we just use 3, because more cleaning data may result in bad performance."],"metadata":{"id":"MrTOfHPiM75m"}},{"cell_type":"code","source":["import re\n","\n","stopwords = set(stopwords.words(\"english\"))\n","\n","# remove http and symbols\n","# reg_map = {\n","#     re.compile(\"rt [@0-9a-z_]{0,10}:\"),\n","#     re.compile(\"#[0-9a-z]+\"),\n","# }\n","reg_http = re.compile(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\")\n","reg_user = re.compile(\"@[0-9a-z]+\")\n","\n","def lower_and_remove_with_reg(text: str) -> str:\n","    # text = text.lower()\n","    # for v in reg_map:\n","    #     text = v.sub(\"\", text)\n","    text = reg_user.sub(\"@user\", text)\n","    text = reg_http.sub(\"http\", text)\n","    return text\n","\n","\n","# clean emoji\n","def clean_emoji(text):\n","    return text.encode(\"ascii\", \"ignore\").decode(\"ascii\")\n","\n","\n","# lemma, remove stopwords\n","def tokenize_and_remove_stopwords(text: str) -> str:\n","    for word in TweetTokenizer().tokenize(text):\n","        word = word.strip(\"'_\")\n","        if word not in stopwords:\n","            yield word\n","\n","\n","def word_check(word: str) -> bool:\n","    if not word:\n","        return\n","\n","    if word.__len__() <= 2:\n","        return\n","    return True\n","\n","\n","def lemmatizing(word: str) -> str:\n","    return WordNetLemmatizer().lemmatize(word, pos=\"v\")\n","\n","\n","# this one is used in word cloud!!!\n","def pre_process(line: str) -> list:\n","    # too many preprocessing \n","    line = clean_emoji(line)\n","    line = lower_and_remove_with_reg(line)\n","    words = filter(word_check, tokenize_and_remove_stopwords(line))\n","    words = map(lemmatizing, words)\n","    return list(words)\n","\n","\n","# What we used in training\n","def pre_process_train(text):\n","  new_text = []\n","  for t in text.split(\" \"):\n","      t = '@user' if t.startswith('@') and len(t) > 1 else t\n","      t = 'http' if t.startswith('http') else t\n","      new_text.append(t)\n","  return \" \".join(new_text)"],"metadata":{"id":"8v8PSaIiLmnM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data Load and Save\n","Save the data to the pickle files.\\\n","Each data is a list of json, each json is a tweet. Tweets are ordered by datetime.\\\n","This step used \"pickleData.ipynb\". We just directly load pickle data here."],"metadata":{"id":"Bl635KgwNjfr"}},{"cell_type":"code","source":["import pickle\n","import pandas as pd\n","\n","\n","def extract_text(tweets):\n","    texts = \"\"\n","    for tweet in tweets:\n","        text = tweet[\"text\"]\n","        new_text = pre_process(text)\n","        string = \" \".join(new_text)\n","        texts += string\n","    return texts\n","\n","\n","def extract_text_train(tweets):\n","    texts = []\n","    for tweet in tweets:\n","        text = tweet[\"text\"]\n","        new_text = pre_process_train(text)\n","        texts.append(new_text)\n","    return texts\n","\n","# load train, dev, test to pandas\n","train_file = open(\"project-data/train_data_pickle\", \"rb\")\n","train_label_file = open(\"project-data/train_label_pickle\", \"rb\")\n","dev_file = open(\"project-data/dev_data_pickle\", \"rb\")\n","dev_label_file = open(\"project-data/dev_label_pickle\", \"rb\")\n","test_file = open(\"project-data/test_data_pickle\", \"rb\")\n","\n","train_list = pickle.load(train_file)\n","train_labels = pickle.load(train_label_file)\n","train_string = []\n","for tweets in train_list:\n","    train_string.append(extract_text(tweets))\n","\n","dev_list = pickle.load(dev_file)\n","dev_labels = pickle.load(dev_label_file)\n","dev_string = []\n","for tweets in dev_list:\n","    dev_string.append(extract_text(tweets))\n","\n","test_list = pickle.load(test_file)\n","test_string = []\n","for tweets in test_list:\n","    test_string.append(extract_text(tweets))\n","\n","# for train\n","train_data = []\n","dev_data = []\n","test_data = []\n","\n","for tweets in train_list:\n","    train_data.append(extract_text_train(tweets))\n","for tweets in dev_list:\n","    dev_data.append(extract_text_train(tweets))\n","for tweets in test_list:\n","    test_data.append(extract_text_train(tweets))"],"metadata":{"id":"Y5IAJWNDNq-u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["covid_file = open(\"project-data/covid_data_pickle\", \"rb\")\n","covid_raw_list = pickle.load(covid_file)\n","covid_raw_data = []\n","for tweets in covid_raw_list:\n","  covid_raw_data.append(extract_text(tweets))"],"metadata":{"id":"_0JzWh3QkyrU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["All the data were cleaned and ready for machine learning baseline."],"metadata":{"id":"d2yknGqskRPv"}},{"cell_type":"code","source":["# both lists, with columns specified\n","\n","pd.set_option('display.width', None)\n","pd.set_option('display.max_colwidth', -1)\n","\n","train_df = pd.DataFrame(list(zip(train_string, train_labels)),columns =['tweet', 'label'])\n","print(train_df.sample(5))\n","dev_df = pd.DataFrame(list(zip(dev_string, dev_labels)),columns =['tweet', 'label'])\n","print(dev_df.sample(5))\n","test_df = pd.DataFrame({'tweet':test_string})\n","print(test_df.sample(5))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ubOIYcmxjOyt","outputId":"b5696c53-eec8-4e4e-f395-1278f7c2e1a0","executionInfo":{"status":"ok","timestamp":1654689357537,"user_tz":-480,"elapsed":370,"user":{"displayName":"Yifan Deng","userId":"12140207404720131325"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     tweet  \\\n","1273  Can regularly rinse nose saline help prevent infection new #coronavirus httpCan gargle mouthwash protect infection new #coronavirus httpCan eat garlic help prevent infection new #coronavirus httpDoes put sesame oil block new #coronavirus enter body httpDoes new #coronavirus affect older people younger people also susceptible httpAre antibiotics effective prevent treat new #coronavirus httpAre specific medicine prevent treat new #coronavirus http                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n","913   1/2 Seems people think Covid like Flu far transmissible require much lesser viral load order infect people The incidence become seriously ill require hospital treatment astronomically greater2/2 require hospital treatment likelihood die also astronomically greater STOP compare stats virus even leave start block yet several versions flu run course year                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n","488   Farting burn calories practical weight loss mechanism http http@user people really thick fuck@Macko1968 @user could see@user@user damn drop today ... smh@user @LaurakBuzz people believe one@user @LaurakBuzz never even hear ... see anyone would believe@user people need read probably poot@user great back draw board@user @LaurakBuzz weigh 120@user technically weight loss could collect gas ...@user @BigKLuv scientifically prove                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n","973   @DFisman @AndreBeltempo @ASPphysician @BogochIsaac @user Also issue happen RHs know They know keep get deprioritized@MoriartyLab @DFisman @AndreBeltempo @ASPphysician @BogochIsaac @user Yet order take advantage new visitor guidelines visitors must get test every two weeks even though staff residents Evidence-based safety measure deterrent limit visitors You decide@Plibra123 @MoriartyLab @DFisman @AndreBeltempo @ASPphysician @BogochIsaac When Will Mandarins Learn That PCR Test Snapshot Time@user @Plibra123 @MoriartyLab @DFisman @ASPphysician @BogochIsaac But think they'd least listen people literally spend days think best way utilize PCR serology Cause look way right What Quebec PCR day What Ont 20k http@AndreBeltempo @Plibra123 @MoriartyLab @DFisman @ASPphysician @BogochIsaac Youd think Thats problem nut right                                                                                  \n","1225  Live White House ... rainbow color Hundreds people ... Hundreds http@CapehartJ You periscope@CapehartJ Left yesterday Really wish today What beautiful sight smile@CapehartJ ominously tip Washington Monument loom like demonic klansman #DebbieDownerhope get selfie @CapehartJ@CapehartJ What mind blow day@user @user Topped Eye Sauron@CapehartJ @user http@CapehartJ @user Forget politics social significance moment-is tacky Your thoughts ...@CapehartJ @user That's great Now let's move give #Iran nuclear bomb we're role Maybe #SCOTUS pass@CapehartJ @MarthaMyrick Really great hear #AdrianRogers sermon even Truly timely selection Book Judges #POTUS #SCOTUS@CapehartJ proud today American@CapehartJ Confusing ...@CapehartJ Thank You Jonathan leadership gay right@CapehartJ pro salute@user @CapehartJ Leviticus also forbid eat clam sell land charge interest trim beard yall stay gays@user @CapehartJ http   \n","\n","      label  \n","1273  0      \n","913   1      \n","488   1      \n","973   0      \n","1225  0      \n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  tweet  \\\n","56   Had meet PMC Bank Depositors http@user_sule Thank madam expect result When #pmcbankcrisis wil resolve Pls consider plight #pmcbankdepositors Mera Bharath Mahan                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n","209  Paula Deen Forget Food Network I've already offer cook show new Fox News Food Channel@TeaPartyCat Paula Deen Forget Food Network ... offer cook show new Fox News Food Channel http@TeaPartyCat cook bull crap rat bits@Spectricide @TeaPartyCat long food network renew contract@TeaPartyCat freedom fry day everyday@TeaPartyCat Paula Deen Forget Food Network I've already offer cook show new Fox News Food Channel lol@TeaPartyCat First episode Paula Deen Fox cook show The Meals Marge Schott@user @TeaPartyCat NSA keep America safe ideas need protect release part theWarOnYankeeHorror@TeaPartyCat Fox News would make terrible food network All table slant right food ingredients slide floor ...@TeaPartyCat Paula Deen Forget Food Network I've already offer cook show new Fox News Food Channel   \n","521  @user @user solution Are say herd immunity take place get virus weaker                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n","335  people say COVID like flu make big deal immediate risk still consider low Americans COVID still evolve moment symptoms range common cold potentially death 1/4Older people people age severe underlie health condition seem higher risk develop serious illness doctor still dont complete picture CONVD 2/4healthcare system cannot handle influx people may expose coronavirus want test treat virus healthcare company say provide free test virus 3/4But moment NOT treatment COVID could bring Economy recession And hundreds millions people live paycheck paycheck easy time could year vaccine create give public 4/4                                                                                                                                                                                        \n","348  safe receive letter package China Yes safe People receive package China risk contract new #coronavirus #Disease #WuhanVirus #COVID19 #QuikTake qt-covid 19-021 #BorderObserver http                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n","\n","     label  \n","56   0      \n","209  1      \n","521  0      \n","335  1      \n","348  0      \n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           tweet\n","301  What Coronavirus #CoronaChainScare #CoronavirusPandemic #coronavirus #COVID19 #StayAtHome #StayHomeStaySafe httpWhat COVID httpWhat COVID symptoms httpWhat COVID symptoms httpWho's risk httpWhat http                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n","417  Every MSP @ScotTories party silent hours longer The #torycivilwar must near http@BAKESY61 @ScotTories @Jackson_Carlaw @AnnieWellsMSP hear pay talktalk broadband account Internet suspend                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n","184  Corona Virus finally confirm Kenya Wondering stay safe Here's protect httpAre consider buy mask love ones protection COVID According WHO httpThis cannot emphasize ... Wash hand frequently Why Washing hand soap water use httpThe way truly reduce spread COVID social distance keep people home much possi httpQuarantine use people sick similar isolation use whe httpbest prepare weeks cut world Here tip pick thos httpHands main pathways germ transmission health care Hand hygiene important especially httpThis go employers ... even COVID arrive operate They reduce work days lose httpNot everyone work home time Part time work home better none Ask httpThe virus move people move stop move virus stop move virus die It's simple #coronaviruskeBased couple myths go round COVID around globe important get facts right esp httpHow new coronavirus affect people get http Informed ... Stay Safe #EpukaCoronavaccine would nice eventually arrive But meantime COVID prevent increase httpDon't share hand others Stay home feel ill Self-isolate continuous cough Its httpThe way truly reduce spread COVID country social distance keep people home httpClean hand rub alcohol-based formulation prefer mean routine hand hygiene httpAvoid touch eye nose mouth Why Hands touch many surface pick viruses Once contaminate han http\n","556  @DrTedros @WHOWPRO @WHOSEARO @WHO_Europe @user @WHOEMRO @WHOAFRO Who members advisers Inte http@DrTedros @WHOWPRO @WHOSEARO @WHO_Europe @user @WHOEMRO @WHOAFRO How members International Heal http@DrTedros @WHOWPRO @WHOSEARO @WHO_Europe @user @WHOEMRO @WHOAFRO How people appoint expert advis http@user @WHO @DrTedros @WHOWPRO @WHOSEARO @WHO_Europe @user @WHOEMRO @WHOAFRO Hope gonna better soon@WHO @DrTedros @WHOWPRO @WHOSEARO @WHO_Europe @user @WHOEMRO @WHOAFRO Thanks WHO Committee China take http                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n","423  Haron Monis Hostage-taker #Sydney Fled #Iran suspect ex-wife kill sex assault http http@Joyce_Karam @user_92 Les #Australiens sont coupables ils auraient expulser preneur d'otage #Iran depuis longtemps@Joyce_Karam Yeah read joker morning allege murder sex assault allegations really eyebrow raise@Joyce_Karam @user Apparently bail Some judge question answer@Joyce_Karam @HalaGorani simple word Mentle@Joyce_Karam @LibyaLiberty Lots fancy word use describe say sick derange expletive look suicide cop@Joyce_Karam @LibyaLiberty Why mad man behind bar Ugh@Joyce_Karam And #Australia patronize hate #Iran guess @SBGillani@Joyce_Karam @user sex assault murder wife AND STILL WALKING AROUND Unbelievable talk war women ...@Joyce_Karam @user BASTARD whine torture prison Didn't jump death burn death behead                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n","  after removing the cwd from sys.path.\n"]}]},{"cell_type":"markdown","source":["Count the token length in each data."],"metadata":{"id":"Fsnu1Rgcyv6N"}},{"cell_type":"code","source":["count = {}\n","for data in train_string:\n","  count[len(data.split())] = count.get(len(data.split()),0) + 1\n","\n","c = 0\n","for k,v in count.items():\n","  if 250 > k:\n","    c=c+v\n","print(c/len(train_string))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oxsU1aEronRm","outputId":"ca495888-7721-4334-ed5c-18e95e76e599"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.8926332288401254\n"]}]},{"cell_type":"markdown","source":["Create the Dataset for deep learning model."],"metadata":{"id":"Nc-Qi8unhD3a"}},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset\n","\n","\n","class TweetDataset(Dataset):\n","    \"\"\"\n","    Used for loading data\n","    \"\"\"\n","\n","    def __init__(self, tokenizer, maxlen, data_type, label=True):\n","\n","        # Initialize the tokenizer\n","        self.tokenizer = tokenizer\n","        self.maxlen = maxlen\n","\n","        # train or dev\n","        self.data_type = data_type  \n","        self.data_list = []\n","        self.label_list = []\n","        self.label = label\n","        \n","        # open pickle file\n","        if self.label:\n","            label_file = open(\"project-data/\" + data_type + \"_label_pickle\", \"rb\")\n","            self.label_list = pickle.load(label_file)\n","        \n","        data_file = open(\"project-data/\" + data_type + \"_data_pickle\", \"rb\")\n","        self.data_list = pickle.load(data_file)\n","        \n","\n","    def __len__(self):\n","        return len(self.data_list)\n","\n","    def __getitem__(self, index):\n","\n","        # get set of corresponding tweets(source and retweet)\n","        tweets = self.data_list[index]\n","        input_text = []\n","\n","        if self.label:\n","            label = self.label_list[index]\n","\n","        # Preprocessing\n","        # texts is a list contains several string     \n","        texts = extract_text(tweets)\n","\n","        # separate every strings with </s>\n","        input_text.append(self.tokenizer.sep_token.join(texts))\n","\n","        encoding = self.tokenizer(\n","            input_text,\n","            max_length=self.maxlen,\n","            padding='max_length',\n","            return_tensors=\"pt\",\n","            truncation=True,\n","        )\n","\n","        if not self.label:\n","            return encoding.input_ids, encoding.attention_mask\n","\n","        # size (1, max_length)\n","        return encoding.input_ids, encoding.attention_mask, label"],"metadata":{"id":"yxUgfgG0hGGE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Base line Model\n","Use sk TF-IDF and NB CLS, since NB is a good classification model."],"metadata":{"id":"R_k-snYdkzVI"}},{"cell_type":"code","source":["%%time\n","import numpy as np\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","\n","X_train = np.array(train_string)\n","X_dev = np.array(dev_string)\n","X_test = np.array(test_string)\n","y_train = np.array(train_labels)\n","y_dev = np.array(dev_labels)\n","\n","tfidf = TfidfVectorizer(ngram_range=(1,3))\n","X_train_tfidf = tfidf.fit_transform(X_train)\n","X_dev_tfidf = tfidf.transform(X_dev)\n","X_test_tfidf = tfidf.transform(X_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lyebp5ZTlX3W","outputId":"dc114b18-94b3-4264-f2b7-ee8b806869f5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 2.84 s, sys: 218 ms, total: 3.05 s\n","Wall time: 5.55 s\n"]}]},{"cell_type":"markdown","source":["Simply tuning the hyperparam of Multi NB."],"metadata":{"id":"ZjxMD07esLVf"}},{"cell_type":"code","source":["from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.naive_bayes import BernoulliNB\n","\n","# nb_cls = MultinomialNB()\n","nb_cls = BernoulliNB()\n","\n","params = {'alpha': (1.5, 1, 0.5, 0.1, 0.01, 0.001, 0.0001, 0.00001)}\n","kf = StratifiedKFold(5, shuffle=True, random_state=1)\n","gs_NB = GridSearchCV(estimator=nb_cls, param_grid=params, cv=kf, scoring='f1') \n","gs_NB.fit(X_train_tfidf, y_train)\n","\n","gs_NB.best_params_"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SeNfMkn0nmIs","outputId":"2cc1e750-3f35-4c0a-d31b-1c00c727858e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'alpha': 1e-05}"]},"metadata":{},"execution_count":50}]},{"cell_type":"markdown","source":["Evaluate on Dev Set. Use AUC, since it tells how much the model is capable of distinguishing between classes. Higher the AUC, the better the model is at predicting 0 classes as 0 and 1 classes as 1. "],"metadata":{"id":"BoYXVAbMs1Xs"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, roc_curve, auc, f1_score\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n","\n","\n","def evaluate_nb(probs, y_true):\n","    \"\"\"\n","    output the roc plot\n","    \"\"\"\n","    preds = probs[:, 1]\n","    fpr, tpr, threshold = roc_curve(y_true, preds)\n","    roc_auc = auc(fpr, tpr)\n","    print(f'AUC: {roc_auc:.4f}')\n","       \n","    # Get accuracy over the test set\n","    y_pred = np.where(preds >= 0.5, 1,0)\n","    accuracy = accuracy_score(y_true, y_pred)\n","    print(f'Accuracy: {accuracy:.5f}')\n","    \n","    f1 = f1_score(y_true, y_pred)\n","    print(f'F1: {f1:.5f}')\n","\n","    # Plot ROC AUC\n","    plt.title('ROC')\n","    plt.plot(fpr, tpr, 'b', label = 'AUC = %.3f' % roc_auc)\n","    plt.legend(loc = 'lower right')\n","    plt.plot([0, 1], [0, 1],'r--')\n","    plt.xlim([0, 1])\n","    plt.ylim([0, 1])\n","    plt.ylabel('True Positive Rate')\n","    plt.xlabel('False Positive Rate')\n","    plt.show()"],"metadata":{"id":"k6BTgaqSs4Bk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nb_model = MultinomialNB(alpha=0.1)\n","nb_model.fit(X_train_tfidf, y_train)\n","probs = nb_model.predict_proba(X_dev_tfidf)\n","\n","\n","# Evaluate the classifier\n","evaluate_nb(probs, y_dev)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":349},"id":"60_322lptbhP","outputId":"ddd1590b-c055-4456-a62e-3857481b4700"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["AUC: 0.9651\n","Accuracy: 0.85981\n","F1: 0.51613\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["predict test data.\\\n","Kaggle: 0.57142"],"metadata":{"id":"yFvXR2Fc1Zhd"}},{"cell_type":"code","source":["# save predict to csv\n","test_pred = nb_model.predict(X_test_tfidf)\n","df = pd.DataFrame(test_pred)\n","df.to_csv(\"submission_baseline.csv\", header=[\"Predicted\"],index=True) "],"metadata":{"id":"QvHNJYM41cmU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Deep learning Model"],"metadata":{"id":"yMffmhJeksaM"}},{"cell_type":"markdown","source":["Before loading dataset to program, we need a Configuration which helps us to modify the parameters like max_length or tokenizer types..."],"metadata":{"id":"eicdHluWSKO_"}},{"cell_type":"code","source":["import numpy as np\n","import torch.nn as nn\n","from transformers import AutoConfig\n","\n","\n","class Config(object):\n","    \"\"\"Config Parameters\"\"\"\n","\n","    def __init__(self):\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # set device\n","        # self.require_improvement = 100  # if nothing imporved when reached 100 epoch, end the proram\n","        self.num_classes = 2  \n","        self.pad_size = 400 # max length\n","        self.hidden_size = 768  # models hidden size, 768 is the size of most pretrained model\n","        self.rnn_hidden = 768  # used for rnn model\n","        self.num_layers = 2  # used for rnn model\n","        self.MODEL = 'roberta_pretrained' # pretrained model\n","        self.out_hidden = AutoConfig.from_pretrained(self.MODEL, output_hidden_states=False) # config pretrained model to output hidden layers\n","        self.dropout = 0.1\n","\n","        # config about training\n","        self.num_epochs = 30\n","        self.batch_size = 25 \n","        self.learning_rate = 5e-5\n","        self.criterion = nn.CrossEntropyLoss()#nn.BCEWithLogitsLoss()  # nn.MSELoss()\n","\n","\n","np.random.seed(1)\n","torch.manual_seed(1)\n","torch.cuda.manual_seed(1)\n","config = Config()\n"],"metadata":{"id":"2smFXYg3SfWb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Loading Data to program."],"metadata":{"id":"gACk_LnAR7I1"}},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","# get the tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(config.MODEL)\n","\n","train_set = TweetDataset(tokenizer=tokenizer, maxlen=config.pad_size, data_type=\"train\") \n","dev_set = TweetDataset(tokenizer=tokenizer, maxlen=config.pad_size, data_type=\"dev\") \n","test_set = TweetDataset(tokenizer=tokenizer, maxlen=config.pad_size, data_type=\"test\",label=False)\n","covid_set = TweetDataset(tokenizer=tokenizer, maxlen=config.pad_size, data_type=\"covid\",label=False)\n","\n","print(\"Done for preprocssing\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F8K2Nl3FR-Hr","outputId":"dfd7c66c-4963-4fd0-e0ff-5053912c81c7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Done for preprocssing\n"]}]},{"cell_type":"markdown","source":["# Create Model\n","1. Bert + FC\n","\n","        Dev acc: 0.9384, f1: 0.8543\n","        Kaggle f1: 0.82105"],"metadata":{"id":"uTjbQGz1UWhL"}},{"cell_type":"code","source":["from transformers import AutoModel\n","\n","\n","class BASE_MODEL((nn.Module)):\n","    def __init__(self, config):\n","        super(BASE_MODEL, self).__init__()\n","        self.roberta = AutoModel.from_pretrained(config.MODEL)\n","        self.fc = nn.Linear(config.rnn_hidden, config.num_classes)\n","\n","    def forward(self, seq, attn_mask):\n","        features = self.roberta(seq, attention_mask=attn_mask)  # seq, attn_mask\n","        out = self.fc(features.last_hidden_state[:,0])\n","        return out\n","    "],"metadata":{"id":"8oEA_iyED4cO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. Roberta + gru\n","        Dev acc: 0.9347014, f1:0.8568\n","        Kaggle f1: 0.8775"],"metadata":{"id":"M-RXxt-bD6sL"}},{"cell_type":"code","source":["class ROBETA_GRU_MODEL((nn.Module)):\n","    def __init__(self, config):\n","        super(ROBETA_GRU_MODEL, self).__init__()\n","        self.roberta = AutoModel.from_pretrained(config.MODEL)\n","\n","        self.gru = nn.GRU(config.hidden_size, config.rnn_hidden, config.num_layers, batch_first=True,\n","                          dropout=config.dropout)\n","\n","        self.fc_rnn = nn.Linear(config.rnn_hidden, config.num_classes)\n","\n","    def forward(self, seq, attn_mask):\n","        features = self.roberta(seq, attention_mask=attn_mask)  # seq, attn_mask\n","        out, hn = self.gru(features.last_hidden_state)\n","        out = self.fc_rnn(hn[-1, :, :])\n","        return out\n"],"metadata":{"id":"NLlfCPwJEc_m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. Robert + sentence + gru cell\n","        Dev acc: 0.94216, f1:0.8669\n","        Kaggle f1: 0.8599"],"metadata":{"id":"5OSirRfjE3d4"}},{"cell_type":"code","source":["from transformers import AutoModel\n","import torch.nn.functional as F\n","\n","\n","class ROBETA_GRUCELL_MODEL((nn.Module)):\n","    def __init__(self, config):\n","        super(ROBETA_GRUCELL_MODEL, self).__init__()\n","        self.roberta = AutoModel.from_pretrained(config.MODEL)\n","        self.gru = nn.GRUCell(config.hidden_size, config.rnn_hidden)\n","        self.fc_rnn = nn.Linear(config.rnn_hidden, config.num_classes)\n","      \n","\n","    def forward(self, seq, attn_mask):\n","        features = self.roberta(seq, attention_mask=attn_mask)  # seq, attn_mask\n","        last_state = features.last_hidden_state\n","        hiddens = []\n","        ids = seq.tolist()\n","\n","        # go through all the batch\n","        for b in range(seq.size(0)):\n","            # ith batch, source tweet vector\n","            hidden = last_state[b, 0, :].view(1, -1)\n","            for i, j in enumerate(ids[b]):\n","                # sentence idx, token j\n","                if j == 2:\n","                    hidden = self.gru(last_state[b, i, :].view(1, -1), hidden)\n","            hiddens.append(hidden)\n","            \n","        logits = self.fc_rnn(torch.cat(hiddens, 0))\n","        \n","        return logits\n"],"metadata":{"id":"wJs06l54FakX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["4. Roberta + CNN\n","        Dev acc: 0.9439, f1:0.8623\n","        Kaggle f1: 0.83516"],"metadata":{"id":"Lj_31WXEF4L8"}},{"cell_type":"code","source":["from transformers import AutoConfig, AutoModel\n","import torch.nn.functional as F\n","\n","class ROBETA_CNN_MODEL((nn.Module)):\n","    def __init__(self, config):\n","        super(ROBETA_GRU_MODEL, self).__init__()\n","\n","        # set roberta to output hidden layers\n","        self.roberta = AutoModel.from_pretrained(config.MODEL,config=config.out_hidden)\n","        self.filter_sizes=[3, 4, 5]\n","        self.num_filters=[100, 100, 100]\n","        self.conv1d_list = nn.ModuleList([\n","            nn.Conv1d(in_channels=config.hidden_size,\n","                      out_channels=self.num_filters[i],\n","                      kernel_size=self.filter_sizes[i])\n","            for i in range(len(self.filter_sizes))\n","        ])\n","        # Fully-connected layer and Dropout\n","        self.fc = nn.Linear(np.sum(self.num_filters), config.num_classes)\n","        self.dropout = nn.Dropout(p=config.dropout)\n","\n","\n","    def forward(self, seq, attn_mask):\n","\n","        # get the output and hidden layers\n","        features = self.roberta(seq, attention_mask=attn_mask)\n","        # change to (batch, embedding, max_length)\n","        last_hidden_reshape = features.last_hidden_state.permute(0, 2, 1)\n","\n","        # Apply CNN and ReLU. Output shape: (b, num_filters[i], L_out)\n","        conv_list = [F.relu(conv1d(last_hidden_reshape)) for conv1d in self.conv1d_list]\n","\n","        # Max pooling. Output shape: (b, num_filters[i], 1)\n","        pool_list = [F.max_pool1d(conv, kernel_size=conv.shape[2]) for conv in conv_list]\n","        \n","        # Concatenate x_pool_list to feed the fully connected layer.\n","        # Output shape: (b, sum(num_filters))\n","        fc = torch.cat([pool.squeeze(dim=2) for pool in pool_list], dim=1)\n","        \n","        # Compute logits. Output shape: (b, n_classes)\n","        logits = self.fc(self.dropout(fc))\n","        \n","        return logits\n","\n"],"metadata":{"id":"iVh2hpEtF32x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["5. Sentence Pairing\n","          Dev Acc: 0.94578 F1: 0.8687\n","          Kaggle：0.89361\n"],"metadata":{"id":"Zo8JA9BIvBZI"}},{"cell_type":"code","source":["from transformers import AutoConfig, AutoModel\n","\n","\n","class ROBETA_SPAIR_MODEL((nn.Module)):\n","    def __init__(self, config):\n","        super(ROBETA_SPAIR_MODEL, self).__init__()\n","\n","        # set roberta to output hidden layers\n","        self.roberta = AutoModel.from_pretrained(config.MODEL,config=config.out_hidden)\n","        self.maxpool = nn.MaxPool1d(3, stride=2, padding=1)\n","        # catch each sentence pair\n","        # self.dropout = nn.Dropout(p=0.5)\n","        self.tanh = nn.Tanh()\n","        # self.linear = nn.Linear(config.hidden_size * 2, config.hidden_size)\n","        self.evidence = nn.Linear(config.hidden_size * 2, config.num_classes)\n","\n","\n","\n","    def forward(self, seq, attn_mask):\n","\n","        # get the output and hidden layers\n","        features = self.roberta(seq, attention_mask=attn_mask)\n","        \n","        logits = []\n","        input_ids = seq.tolist()\n","\n","        # 8,256,768\n","        # go through every data in batch\n","        for b in range(seq.size(0)):\n","\n","            # 256,768\n","            sen_vec_index = []\n","            # go through each token id in one data\n","            for i, j in enumerate(input_ids[b]):\n","                # 2 is </s> id\n","                if j == 2:\n","                    sen_vec_index.append(i)\n","          \n","            # num of tweets, 768\n","            # according index to extract sentence\n","            sen_vec = torch.index_select(features.last_hidden_state[b, :, :], 0, torch.Tensor(sen_vec_index).type_as(seq).long())\n","            # convert to 1 row 768 col,and repeat size time, become size * 768\n","            first_sen = sen_vec[0].view(1, -1).repeat(sen_vec.size(0), 1)\n","\n","            # num of tweets, 768*2  \n","            pair = torch.cat([first_sen, sen_vec], 1)\n","            # num of tweets, 768\n","            logit = self.tanh(pair)\n","            \n","            # should be\n","            # logit = self.dropout(self.tanh(self.linear(pair)))\n","\n","            # num of tweets,2\n","            logit = self.evidence(logit)\n","            # 2， 用mean是不确定有歧义的两个句子表现出来的值是什么样的\n","            logits.append(torch.mean(logit, 0))\n","        # 8,2\n","        logits = torch.stack(logits, 0)\n","        # logits = torch.nn.functional.softmax(logits)\n","        return logits"],"metadata":{"id":"7zdDUWUCu-ZP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Use built model, and set optimizer for training."],"metadata":{"id":"jjkzGb82chfZ"}},{"cell_type":"code","source":["import torch.optim as optim\n","\n","model = ROBETA_GRUCELL_MODEL(config)\n","model.to(config.device)\n","\n","# set dynamic learning rate and L2, help to fit the training set but avoid overfitting\n","opti = optim.Adam(model.parameters(), lr = config.learning_rate, weight_decay=1e-3)\n","scheduler = optim.lr_scheduler.StepLR(opti, step_size=15, gamma=0.5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5C7B63rNcp_i","outputId":"628ded83-8d0d-4534-abd5-9f7fe53c5dab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta_pretrained were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta_pretrained and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"markdown","source":["Create batch data use dataloader."],"metadata":{"id":"2wua1krGeMSH"}},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","train_loader = DataLoader(train_set, batch_size=config.batch_size, shuffle=True)\n","dev_loader = DataLoader(dev_set, batch_size=config.batch_size, shuffle=True)\n","test_loader = DataLoader(test_set, batch_size=config.batch_size)\n","covid_loader = DataLoader(covid_set, batch_size=config.batch_size)\n","print(\"Data Loaded.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nXX6t1gAeL-8","outputId":"83903d1c-6409-4f93-f13d-a1e496d04b50"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Data Loaded.\n"]}]},{"cell_type":"markdown","source":["# Train Model\n","Create evaluate function for bp, it contains accuracy, precision, recall and f1."],"metadata":{"id":"CjojYfTheiS-"}},{"cell_type":"code","source":["from sklearn.metrics import precision_recall_fscore_support\n","\n","\n","def get_accuracy_from_logits(logits, labels):\n","    # print(\"logits: \", logits)\n","    # use sigmoid to do binary cls\n","    probs = torch.argmax(logits.squeeze(), dim=1)\n","    print(\"predict: \",probs)\n","    print(\"labels: \",labels)\n","    acc = (probs.squeeze() == labels).float().mean()\n","    return acc\n","\n","\n","def evaluate(net, dataloader, ep):\n","    net.eval()\n","\n","    mean_acc, mean_loss = 0, 0\n","    count = 0\n","    predicted = []\n","    true_label = []\n","    with torch.no_grad():\n","        for seq, attn_masks, labels in dataloader:\n","            \n","            # caculate the accuracy and loss for every batch\n","            seq, attn_masks, labels = seq.to(config.device), attn_masks.to(config.device), labels.to(config.device)\n","            logits = net(seq.squeeze(1), attn_masks)\n","            mean_loss += nn.functional.cross_entropy(logits.squeeze(-1), labels.type(torch.LongTensor).to(config.device) , reduction='mean')\n","            mean_acc += get_accuracy_from_logits(logits, labels)\n","            count += 1\n","\n","            # record every p r f1\n","            true_label.extend(labels.tolist())\n","            probs = torch.argmax(logits.squeeze(), dim=1)\n","            tmp = (probs.squeeze() == labels).float().tolist()\n","            predicted.extend(tmp)\n","\n","        with dev_summary_writer.as_default():\n","            summary.scalar('loss', (mean_loss/count).data.cpu().numpy(), step=ep)\n","            summary.scalar('accuracy', (mean_acc/count).data.cpu().numpy(), step=ep)\n","            \n","    p, r, f, _ = precision_recall_fscore_support(true_label, predicted, pos_label=1, average=\"macro\")\n","    return mean_acc / count, mean_loss / count, p, r, f\n"],"metadata":{"id":"skTJ2Wx2ei19"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Train function"],"metadata":{"id":"JYBMf7YywdML"}},{"cell_type":"code","source":["import time\n","\n","\n","def train(net, criterion, opti, train_loader, dev_loader, max_eps, device):\n","    best_acc = 0\n","    st = time.time()\n","    # every epoch\n","    for ep in range(max_eps):\n","\n","        net.train()\n","        # for it, batch_encoding in enumerate(train_loader2):\n","        for it, (seq, attn_masks, labels) in enumerate(train_loader):\n","            # Clear gradients\n","            opti.zero_grad()\n","\n","            # Obtaining the logits from the model\n","            seq, attn_masks, labels = seq.to(config.device), attn_masks.to(config.device), labels.to(config.device)\n","            logits = net(seq.squeeze(1), attn_masks)\n","\n","            # Computing loss\n","            loss = nn.functional.cross_entropy(logits.squeeze(-1), labels.type(torch.LongTensor).to(config.device))\n","\n","            \"\"\"\n","            l2_lambda = 0.001\n","            l2_norm = sum(p.pow(2.0).sum() for p in neta.parameters())\n","            loss = loss + l2_lambda * l2_norm\n","            \"\"\"\n","\n","            # Backpropagating the gradients\n","            loss.backward()\n","\n","            opti.step()\n","\n","            if it % 50 == 0:\n","                acc = get_accuracy_from_logits(logits, labels)\n","                print(\"{},{},{},{}; Time taken (s): {}\".format(it, ep,loss.item(),acc, (time.time() - st)))\n","                st = time.time()\n","\n","        scheduler.step()\n","\n","        # show in tensorboard\n","        with train_summary_writer.as_default():\n","            summary.scalar('loss', loss.item(), step=ep)\n","            \n","        dev_acc, dev_loss, p, r, f = evaluate(net, dev_loader, ep)\n","        print(\"Epoch {} complete! Development Accuracy: {}; Development Loss: {}, Precision: {}, Recall: {}, F1: {}\".format(ep, dev_acc, dev_loss,p, r, f))\n","        \n","        if dev_acc > best_acc:\n","            print(\"Best development accuracy improved from {} to {}, saving model...\".format(best_acc, dev_acc))\n","            best_acc = dev_acc\n","            torch.save(net.state_dict(), 'evidence512_{}.pt'.format(ep))\n"],"metadata":{"id":"482xnrjuwVE9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Enable tensorboard"],"metadata":{"id":"tY1g779dwh9x"}},{"cell_type":"code","source":["from torchvision import transforms, utils, datasets\n","from tensorflow import summary\n","import datetime\n","%load_ext tensorboard\n","\n","current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","train_log_dir = 'logs/tensorboard/train/evidence512-' + current_time\n","dev_log_dir = 'logs/tensorboard/dev/evidence512-' + current_time\n","train_summary_writer = summary.create_file_writer(train_log_dir)\n","dev_summary_writer = summary.create_file_writer(dev_log_dir)"],"metadata":{"id":"1RF0J6XIwiUQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%tensorboard --logdir logs/tensorboard"],"metadata":{"id":"nmE5xSwdwnyn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Train the model"],"metadata":{"id":"uxQIPpBCwsg4"}},{"cell_type":"code","source":["train(model, config.criterion, opti, train_loader, dev_loader, config.num_epochs,device=config.device)"],"metadata":{"id":"hM09S7jSwsRm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Prediction"],"metadata":{"id":"7NCl1m98w6Tb"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd \n","from tqdm import tqdm\n","def predict(net, dataloader):\n","    \"\"\"\n","    Use fine tuned bert to predict (forward)\n","    \"\"\"\n","    net.eval()\n","\n","    logits = []\n","    with torch.no_grad():\n","        # go through each batch\n","        for seq, attn_masks in tqdm(dataloader):\n","            # compute logits\n","            seq, attn_masks = seq.to(config.device), attn_masks.to(config.device)\n","            logit = net(seq.squeeze(1),attn_masks)\n","            logits.append(logit)\n","            \n","    # concatenate batch\n","    logits = torch.cat(logits, dim=0)\n","    probs = torch.argmax(logits.squeeze(), dim=1)\n","\n","    return probs.cpu().numpy()\n"],"metadata":{"id":"o43GHha_wxX6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load the best parameter\n","model_1_batch = ROBETA_GRU_MODEL(config)\n","model_1_batch.load_state_dict(torch.load(\"evidence512.pt\"))\n","model_1_batch.to(config.device)"],"metadata":{"id":"tgCEf-l1xArz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predicts_test = predict(model_1_batch, test_loader)\n","pd.DataFrame(predicts_test).to_csv(\"submission18-eviden400-25.csv\")\n","\n","# predicts_covid = predict(model_1_batch, covid_loader)\n","# pd.DataFrame(predicts_covid).to_csv(\"covid_result_best.csv\")\n"],"metadata":{"id":"ws1yJiW9xRf_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Sentiment Analysis"],"metadata":{"id":"Pw1MOaSGxbFu"}},{"cell_type":"markdown","source":["Predict COVID"],"metadata":{"id":"el8N16V_yNj-"}},{"cell_type":"code","source":["predicts_covid = predict(model_1_batch, covid_loader)\n","pd.DataFrame(predicts_covid).to_csv(\"covid_result_best.csv\")\n","\n","  \n","covid_source = []\n","for i in covid_set:\n","  covid_source.append(i[0])"],"metadata":{"id":"r4eYYip8ySnn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rumour = []\n","for i,j in zip(covid_source,predicts_covid):\n","  if j == 1:\n","    rumour.append(i)\n","print(len(rumour))\n","nonrumour = []\n","for i,j in zip(covid_source,predicts_covid):\n","  if j == 0:\n","    nonrumour.append(i)\n","print(len(nonrumour))"],"metadata":{"id":"FDCG0fJByKDY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","import ssl\n","\n","# try:\n","#     _create_unverified_https_context = ssl._create_unverified_context\n","# except AttributeError:\n","#     pass\n","# else:\n","#     ssl._create_default_https_context = _create_unverified_https_context\n","\n","nltk.download('vader_lexicon')\n","from nltk.sentiment import SentimentIntensityAnalyzer\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","\n","sentiment = SentimentIntensityAnalyzer()\n","\n","# neg, neu, pos\n","rumour_sentiments = [0, 0, 0]\n","for r in rumour:\n","    scores = sentiment.polarity_scores(r)\n","    # neg\n","    if scores[\"neg\"] > scores[\"pos\"] and scores[\"neg\"] > scores[\"neu\"]:\n","        rumour_sentiments[0] += 1\n","    # pos\n","    elif scores[\"pos\"] > scores[\"neg\"] and scores[\"pos\"] > scores[\"neu\"]:\n","        rumour_sentiments[2] += 1\n","    # neu\n","    else:\n","        rumour_sentiments[1] += 1\n","\n","nonrumour_sentiments = [0, 0, 0]\n","for n in nonrumour:\n","    scores = sentiment.polarity_scores(n)\n","    if scores[\"neg\"] > scores[\"pos\"] and scores[\"neg\"] > scores[\"neu\"]:\n","        nonrumour_sentiments[0] += 1\n","    elif scores[\"pos\"] > scores[\"neg\"] and scores[\"pos\"] > scores[\"neu\"]:\n","        nonrumour_sentiments[2] += 1\n","    else:\n","        nonrumour_sentiments[1] += 1\n","\n","\n","rumour_neu_percent = rumour_sentiments[1]/(rumour_sentiments[0]+rumour_sentiments[1]+rumour_sentiments[2])\n","rumour_neg_percent = rumour_sentiments[0]/(rumour_sentiments[0]+rumour_sentiments[2])\n","rumour_pos_percent = rumour_sentiments[2]/(rumour_sentiments[0]+rumour_sentiments[2])\n","\n","\n","nonrumour_neu_percent = nonrumour_sentiments[1]/(nonrumour_sentiments[0]+nonrumour_sentiments[1]+nonrumour_sentiments[2])\n","nonrumour_neg_percent = nonrumour_sentiments[0]/(nonrumour_sentiments[0]+nonrumour_sentiments[2])\n","nonrumour_pos_percent = nonrumour_sentiments[2]/(nonrumour_sentiments[0]+nonrumour_sentiments[2])\n","\n","print(rumour_neu_percent,rumour_neg_percent,rumour_pos_percent)\n","print(nonrumour_neu_percent,nonrumour_neg_percent,nonrumour_pos_percent)"],"metadata":{"id":"nTRCcg7pxa1e"},"execution_count":null,"outputs":[]}]}