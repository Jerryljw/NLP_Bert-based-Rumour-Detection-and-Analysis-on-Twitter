{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pickleData.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMKdgIlAMpzbEcjeZNHpINH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","!ls\n","import os\n","# move to content may be quicker\n","os.chdir('/content/drive/MyDrive/2022S1/90042nlp/proj')\n","!ls\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"86MHxqQ2hzaO","executionInfo":{"status":"ok","timestamp":1651717792307,"user_tz":-480,"elapsed":53303,"user":{"displayName":"Yifan Deng","userId":"12140207404720131325"}},"outputId":"f2a77600-d682-42d1-ec15-ca6a17912e08"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","drive  sample_data\n","BaseBert+GRU.ipynb     proj.ipynb\t\t  ss_1.dat\tsubmission1.csv\n","BERTweet_base_fairseq  pytorch-xla-env-setup.py   sst_0.dat\tsubmission.csv\n","pickleData.ipynb       report.gdoc\t\t  sst_1.dat\ttest.ipynb\n","project-data\t       sentenceTransformer.ipynb  sst_2.dat\n","project.pdf\t       ss_0.dat\t\t\t  sstcls_0.dat\n"]}]},{"cell_type":"code","source":["import datetime\n","import json\n","import pickle"],"metadata":{"id":"7rPXCZGsh-Ba"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def extract_time(json, flag):\n","  if flag == \"v1\":\n","    format = \"%a %b %d %H:%M:%S +0000 %Y\"\n","  else:\n","    format = \"%Y-%m-%dT%H:%M:%S.%fZ\"\n","  try:\n","    # convert created_at time format to datetime format\n","    return datetime.datetime.strptime(json['created_at'], format)\n","  except KeyError:\n","    return 0"],"metadata":{"id":"3DaO8E9MiHgU"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MibWkEf0gU59"},"outputs":[],"source":["# no label v1.0 is test data\n","data_list = []\n","\n","# open id file\n","ids = open(\"project-data/test.data.txt\",\"r\")\n","\n","# open corresponding tweet json files\n","tweet_json_path = \"project-data/tweet-objects/\"\n","\n","# go through every line in txt\n","for ids_line in ids.readlines():\n","  ids_list = ids_line.strip().split(\",\")\n","  json_list = []  # store the content\n","\n","  for ids in ids_list:\n","    # assume all the ids have corresponding text\n","    temp_file_path = tweet_json_path+ids+\".json\"\n","    temp_json = json.load(open(temp_file_path,\"r\"))\n","    json_list.append(temp_json)\n","        \n","  # v1.0 Fri Jun 21 22:24:12 +0000 2013 \n","  json_list.sort(key=lambda x: extract_time(x, \"v1\"))\n","  #print(json_list)\n","  data_list.append(json_list)"]},{"cell_type":"code","source":["outfile = open(\"project-data/test_data_pickle\",'wb')\n","pickle.dump(data_list,outfile)\n","outfile.close()"],"metadata":{"id":"653Z5xJ6iw9c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_list = []\n","label_list = []\n","    \n","# open id file\n","ids = open(\"project-data/train.data.txt\",\"r\")\n","labels = open(\"project-data/train.label.txt\",\"r\")\n","\n","# open corresponding tweet json files\n","tweet_json_path = \"project-data/crawl_data/train_data/\"\n","\n","# go through every line in txt\n","for ids_line, label in zip(ids.readlines(), labels.readlines()):\n","  ids_list = ids_line.strip().split(\",\")\n","  json_list = []  # store the content\n","  # deal with the source tweets that were not crawled\n","  if os.path.exists(tweet_json_path+ids_list[0]+\".json\"):\n","    # if id have tweet content\n","    for ids in ids_list:\n","      # check if the rest tweets were crawled\n","      temp_file_path = tweet_json_path+ids+\".json\"\n","      if os.path.exists(temp_file_path):\n","        temp_json = json.load(open(temp_file_path,\"r\"))\n","        json_list.append(temp_json)\n","        \n","    # v2.0 2020-04-29T17:01:38.000Z \n","    json_list.sort(key=lambda x: extract_time(x, \"v2\"))\n","    #print(json_list)\n","    data_list.append(json_list)\n","    label_list.append(1 if label.strip()==\"rumour\" else 0)"],"metadata":{"id":"QzKd0V1rhxLo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["outfile = open(\"project-data/train_data_pickle\",'wb')\n","outfile1 = open(\"project-data/train_label_pickle\",'wb')\n","pickle.dump(data_list,outfile)\n","pickle.dump(label_list,outfile1)\n","outfile.close()\n","outfile1.close()"],"metadata":{"id":"ev8Q_OOOjH0N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_list = []\n","label_list = []\n","    \n","# open id file\n","ids = open(\"project-data/dev.data.txt\",\"r\")\n","labels = open(\"project-data/dev.label.txt\",\"r\")\n","\n","# open corresponding tweet json files\n","tweet_json_path = \"project-data/crawl_data/dev_data/\"\n","\n","# go through every line in txt\n","for ids_line, label in zip(ids.readlines(), labels.readlines()):\n","  ids_list = ids_line.strip().split(\",\")\n","  json_list = []  # store the content\n","  # deal with the source tweets that were not crawled\n","  if os.path.exists(tweet_json_path+ids_list[0]+\".json\"):\n","    # if id have tweet content\n","    for ids in ids_list:\n","      # check if the rest tweets were crawled\n","      temp_file_path = tweet_json_path+ids+\".json\"\n","      if os.path.exists(temp_file_path):\n","        temp_json = json.load(open(temp_file_path,\"r\"))\n","        json_list.append(temp_json)\n","        \n","    # v2.0 2020-04-29T17:01:38.000Z \n","    json_list.sort(key=lambda x: extract_time(x, \"v2\"))\n","    #print(json_list)\n","    data_list.append(json_list)\n","    label_list.append(1 if label.strip()==\"rumour\" else 0)"],"metadata":{"id":"7nn0ZxByjGEK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["outfile = open(\"project-data/dev_data_pickle\",'wb')\n","outfile1 = open(\"project-data/dev_label_pickle\",'wb')\n","pickle.dump(data_list,outfile)\n","pickle.dump(label_list,outfile1)\n","outfile.close()\n","outfile1.close()"],"metadata":{"id":"oRRbcjfNlDvX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# no label v1.0 is test data\n","data_list = []\n","\n","# open id file\n","ids = open(\"project-data/covid.data.txt\",\"r\")\n","\n","# open corresponding tweet json files\n","tweet_json_path = \"project-data/crawl_data/covid_data/\"\n","\n","# go through every line in txt\n","for ids_line in ids.readlines():\n","  ids_list = ids_line.strip().split(\",\")\n","  json_list = []  # store the content\n","\n","  # deal with the source tweets that were not crawled\n","  if os.path.exists(tweet_json_path+ids_list[0]+\".json\"):\n","    # if id have tweet content\n","    for ids in ids_list:\n","      # check if the rest tweets were crawled\n","      temp_file_path = tweet_json_path+ids+\".json\"\n","      if os.path.exists(temp_file_path):\n","        temp_json = json.load(open(temp_file_path,\"r\"))\n","        json_list.append(temp_json)\n","        \n","    # v1.0 Fri Jun 21 22:24:12 +0000 2013 \n","    json_list.sort(key=lambda x: extract_time(x, \"v2\"))\n","    #print(json_list)\n","    data_list.append(json_list)"],"metadata":{"id":"zvhDxF97mhS8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["outfile = open(\"project-data/covid_data_pickle\",'wb')\n","pickle.dump(data_list,outfile)\n","outfile.close()"],"metadata":{"id":"bkLQBr_rmqKC"},"execution_count":null,"outputs":[]}]}